\documentclass[8pt, letterpaper]{extarticle}
\usepackage[margin=0in]{geometry}  % 0" margins on letter paper
\usepackage{multicol}              % multiple columns
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{wasysym}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{physics}
\usepackage{enumitem}              % to adjust bullet indentation
\usepackage{changepage}
\pagestyle{empty}                  % no page numbers


\usepackage{titlesec}
\usepackage[normalem]{ulem} % for underline that works in headings

\ifPDFTeX % ensure generation of machine readable output
\input{glyphtounicode}
\pdfgentounicode=1
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\fi

\usepackage{csquotes}

\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Nul}{Nul}
\DeclareMathOperator{\Row}{Row}
\DeclareMathOperator{\proj}{proj}

\setlength{\arraycolsep}{12pt}
\setlength{\parindent}{0pt}

\newcommand{\Eg}{\textbf{Eg.}\xspace}
\newcommand{\Ex}{\textbf{Ex.}\xspace}
\newcommand{\Ie}{\textbf{I.e.}\xspace}
\newcommand{\bigEps}{\mathcal{E}}
\newcommand{\bproof}{\textit{Proof ($\impliedby$).}\xspace}
\newcommand{\defn}{\textbf{Def.}\xspace}
\newcommand{\eg}{\textbf{e.g.}\xspace}
\newcommand{\ex}{\textbf{ex.}\xspace}
\newcommand{\fproof}{\textit{Proof ($\implies$).}\xspace}
\newcommand{\ie}{\textbf{i.e.}\xspace}
\newcommand{\lemma}{\textit{Lemma}\xspace}
\newcommand{\soln}{\textit{Soln.}\xspace}
\newcommand{\thm}{\textbf{Thm.}\xspace}

\renewcommand{\arraystretch}{1.25} % Adjust row spacing

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
}

\newcommand{\ulhref}[2]{\href{#1}{\color{blue}\uline{#2}}}

% Redefine \section
\titleformat{\section}
  {\normalfont\bfseries\normalsize} % formatting
  {} % label
  {0pt} % separation between label and title
  {\underline} % before code (underline the title)
\titlespacing*{\section}{0pt}{*1}{*0.5} % {left}{before}{after}

% Redefine \subsection
\titleformat{\subsection}
  {\normalfont\bfseries\normalsize} % formatting
  {} % label
  {0pt} % separation
  {} % no underline
\titlespacing*{\subsection}{0pt}{*0.8}{*0.3}

% Redefine \subsection
\titleformat{\subsubsection}
  {\normalfont\bfseries\small} % formatting
  {} % label
  {1em} % separation
  {} % no underline
\titlespacing*{\subsubsection}{0pt}{*0.8}{*0.3}


\setlist[itemize]{leftmargin=*, labelsep=.2em, parsep=0pt,
itemsep=0.2\baselineskip, partopsep=0pt, topsep=0pt}
\setlist[enumerate]{leftmargin=*, labelsep=.2em, parsep=0pt,
itemsep=0.2\baselineskip, partopsep=0pt, topsep=0pt}
\setlength{\columnsep}{0.25in}
\begin{document}

\title{MACM 316 Cheat Sheet (Final Revision)}
\author{Alexander Ng}
\date{Thursday, April 10, 2025}

% \maketitle

\begin{multicols*}{3}
  \section{Things to Remember}
  \begin{itemize}
    \item $a_{ij}$ is the $i$th row and $j$th column of $A$.
      % maybe remove the thing on decimal floating point
    \item $\pm 0.d_1d_2\dots d_k \times 10^n$ is the decimal floating point 
      representation of a number.
    \item Chopping is cheaper than rounding.
  \end{itemize}

  \subsection{Error}
  \begin{itemize} % consider making this a table
    \item Error: $p - \hat{p}$
    \item Abs. Err: $\abs{p - \hat{p}}$
    \item Rel. Err: $\frac{\abs{p - \hat{p}}}{p}$ (for accuracy)
  \end{itemize}

  \subsection{Significant Digits}
  An approximation $\hat p$ has $t$ significant digits if: \newline
  $\displaystyle \frac{\abs{p-\hat p}}{\abs{p}} \leq 5 \times 10^{-t}$

  \subsection{Catastrophic Cancellation (Roundoff)}
  When subtracting nearly equal numbers, the relative error is large, and you
  lose a lot of significant digits (and accuracy).

  \subsection{How to Reduce Errors}
  \begin{itemize}
    \item Reformat the formula to avoid roundoff
    \item Reduce num. of ops (avoid rounding)
      \begin{itemize}
        \item Nested Arithmetic: Rewrite polynomials to reduce operations
          \newline
          $x^3 - 6.1x^2 + 3.2x \to \pqty{\pqty{x-6.1}x + 3.2}x$
      \end{itemize}
  \end{itemize}

  \subsubsection{Alternative Quadratic Formula}
  \begin{equation*}
  x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} = \frac{-2c}{b \mp \sqrt{b^2 - 4ac}}  
  .\end{equation*}
  \tiny *mind the signs*

  \normalsize
  use the alt. form when $\abs{-b}$ is close to $+ \sqrt{b^2 - 4ac}$

  \section{Algorithms and Convergence}
  \begin{itemize}
    \item Stable $\to$ errors grow linearly
    \item Unstable $\to$ errors grow exponentially
  \end{itemize}

  \subsection{Rate of Convergence}
  \begin{itemize}
    \item For sequences, if $\alpha_n \to \alpha$ and $\abs{\alpha_n - \alpha}
      \leq k \beta_n, \quad \beta_n \to 0$ then $\alpha_n$ is $\order{\beta_n}$
    \item For functions, if $\lim_{h \to 0} f(h) = L$ and $\abs{f(h)} \leq kh^p$
      then $f(h) = L + \order{h^p}$
  \end{itemize}

  \subsection{Taylor Series}
  $\displaystyle f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}(x-a)^n$\newline
  $e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dotsb$\newline
  $\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!}\qquad\cos x = 1 - \frac{x^2}{2!} +
  \frac{x^4}{4!}$\newline
  $\ln (1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} + \dotsb$ \newline % |x| < 1
  $(1+x)^{-p} = 1-px+\frac{p(p+1)x^2}{2} -\frac{p(p+1)(p+2)x^3}{3!}$ % |x| < 1
  \textbf{The Error Term} is the $(n+1)^{th}$ term.

  \section{Root Finding}
  \begin{itemize}
  \item Find $p$ such that $f(p) = 0$.
  \end{itemize}

  \subsection{Generic Stopping Criterion}
  \begin{enumerate}
    \item $\frac{\abs{p_n-p_{n-1}}}{\abs{p_n}} \leq \bigEps; p_n \neq 0:$
      relative error
    \item $\abs{f(p_n)|} \leq \bigEps$
      \begin{itemize}
        \item Ensures small $f(p_n)$
        \item $p_n$ may differ significantly from $p$
      \end{itemize}
    \item Have a fixed number of iterations
    \item (bisection) 
      $\frac{b_n-a_n}{2} \leq \bigEps$ \textbf{or} $\abs{p_n-p_{n-1}} <\bigEps$
      \begin{itemize}
        \item Ensures $p_n$ is within $\bigEps$ of  $p$
        \item Does not ensure small $f(p_n)$
      \end{itemize}
  \end{enumerate}

  \subsection{Bisection Method:}

  \begin{itemize}
    \item \textbf{Conditions:} $f(x) \in C[a,b];$ \newline
      $f(a)$ and $f(b)$ have opposite signs.
    \item \textbf{Midpoint:} $x = \frac{a+b}{2}$
    \item \textbf{Procedure:} Binary search for the root.
    \item \textbf{Error:} Guaranteed quadratic convergence 
    \item \textbf{Error Formula:} $\frac{b-a}{2^n}$
  \end{itemize}

  \subsection{Newton's Method}
  \begin{itemize}
    \item Faster than bisection, quadratic. We follow the tangent line at $p_{n-1}$ to its
      $x-$intercept.
    \item Requires $f'(p)$ to exist.
    \item Requires $f''(p)$ for quadratic convergence.
      \begin{enumerate}
        \item Start with initial guess $p_0$ and $p_1$
        \item $\displaystyle p_n = p_{n-1} -
          \frac{f(p_{n-1})\pqty{p_{n-1}-p_{n-2}}}{f'(p_{n-1})f(p_{n-2})}$
      \end{enumerate}
  \end{itemize}

  \subsection{Secant Method}
  \begin{itemize}
    \item Does not require $f'(p)$ to exist.
    \item Faster than Bisection, order $\phi \approx 1.618$
      \begin{enumerate}
      \item Start with initial guess $p_0$ and $p_1$
      \item $\displaystyle p_n = p_{n-1} -
        \frac{f(p_{n-1})\pqty{p_{n-1}p_{n-2}}}{f(p_{n-1})f(p_{n-2})}$
      \end{enumerate}
  \end{itemize}

  % not tested
  % \subsection{False Position Method}
  % \begin{itemize}
  %   \item Linear or sublinear guaranteed convergence.
  %   \item Convergence can stall if the function has poor behaviour near the root.
  %     \begin{enumerate}
  %       \item Start with initial guess $p_0$ and $p_1$
  %       \item $\displaystyle p_n = p_{n-1} f(p_{n-1}) \cdot \frac{p_{n-1}-p_{n-2}}{f(p_{n-1})-f(p_{n-2})}$
  %     \end{enumerate}
  % \end{itemize}

  \section{Fixed Points}
  \begin{enumerate}
    \item Start with initial guess $p_0$
    \item Generate a sequence $p_n = g(p_{n-1})$
    \item Stop when $\abs{p_n - p_{n-1}} < \bigEps$
  \end{enumerate}
  \begin{itemize}
    \item A fixed point of $f$ is a point $p$ such that $f(p) = p$.
    \item Converges if:
      \begin{enumerate}
        \item $g:[a,b] \to [a,b]$ is continuous
        \item $\forall x \in [a,b]: \abs{g'(x)} \leq k < 1 $
        \item $f(x) = 0$ can be rewritten as $g(x) = x$
      \end{enumerate}
    \item \textbf{Error}: $\order{q^n}$, for some $q$, faster when $q$ is small
  \end{itemize}

  \section{Norms}

  \subsection{Vector Norms}
  \begin{itemize}
    \item $l_1 : \norm{x}_1 = \sum x_i$
    \item $l_2 : \norm{x}_2 = \sqrt{x_1^2 + \dotsb + x_n^2}$ (Euclidean)
    \item $l_\infty : \norm{x}_\infty = \max{\{|x_1|, \dotsb, |x_n|\}}$
      ($\infty$) 
  \end{itemize}

  \subsubsection{Properties}
  \begin{itemize}
    \item Scalability: $\norm{\alpha x} = \abs{\alpha} \norm{x}$
    \item Triangle Inequality: $\norm{x+y} \leq \norm{x} + \norm{y}$
  \end{itemize}

  \subsection{Vector Distances}
  \begin{itemize}
    \item $l_\alpha$ distance: $\norm{x-y}_\alpha$
  \end{itemize}

  \subsection{Matrix Norms}
  \begin{itemize}
    \item The Natural Norm $\norm{\cdot}_*$ for $A, B \in \mathbb{R}^{n \times n};
      \alpha \in \mathbb{R}$ is defined as a function that satisfies:
      \begin{enumerate}
        \item $\norm{A} \geq 0$
        \item $\norm{A} = 0 \iff A = 0$
        \item $\norm{\alpha A} = \abs{\alpha} \norm{A}$
        \item $\norm{A + B} \leq \norm{A} + \norm{B}$
      \end{enumerate}
    \item \defn $\norm{A}_* = \underset{\|x\| = 1}{\max} \norm{Ax}_*$ where
      $\norm{Ax}$ is any vector norm.
    \item $l_\infty : \norm{A}_\infty = \underset{1 \leq i \leq n}{\max}
      \sum_{j=1}^{n} \abs{a_{ij}}$ (row sum)
  \end{itemize}
  \subsubsection{Special Properties}
  \begin{enumerate}
    \item For any natural norm $\norm{\cdot}_\alpha:\rho(A) \leq \norm{A}_\alpha$
    \item For $l_2:\norm{A}_2 = \sqrt{\rho(A^TA)}$
  \end{enumerate}

  \section{Vector Sequence Convergence}
  \begin{itemize}
    \item $\Bqty{x^{(k)}}$ converges to $x$ for any small $\bigEps > 0$
      eventually every $x^{(k)}$ is within $\bigEps$ of $x$
  \end{itemize}

  \section{Eigenvalues and Eigenvectors}
  \textbf{E.value ($\lambda$)}: Scalar s.t. $A\vec{x} = \lambda \vec{x}$ \\
  \textbf{E.vector ($\vec{x}$)}: Nonzero vector only scaled by $A$
  \textbf{Spectral Radius}: $\rho(A) = \max \{\abs{\lambda_i}\}$
  \subsubsection{Properties}
  \begin{enumerate}
    \item $\det(A-\lambda I) = 0 \iff \lambda$ is an eigenvalue. Solve the
      characteristic polynomial for $\lambda$.
    \item $\forall \lambda \bqty{(A-\lambda I)\vec{x} = 0 \iff \vec x \text{ is an
      eigenvector}}$
    \item If $\rho A < 1$, $A$ is \uline{convergent} $\implies 
      \underset{k\to\infty}{\lim} A^k = 0$
  \end{enumerate}

  \section{Linear Systems - Pivoting Strategies}
  \textit{If the pivot is small, large errors can occur. Pivoting helps maintain numerical stability.}

  \subsection{Partial Pivoting}
  Choose the largest element in the \textbf{current column} (below or at the pivot) to avoid dividing by a small number.
  \begin{enumerate}
    \item For $k = 1 \dots n-1$:
      \begin{itemize}
        \item Find $r = \underset{k \leq i \leq n}{\arg\max} \Bqty{|a_{ik}|}$
        \item If $r \neq k$, swap rows: $E_k \leftrightarrow E_r$
        \item Continue Gaussian Elimination as usual
      \end{itemize}
  \end{enumerate}

  \subsection{Scaled Partial Pivoting}
  Handles rows with vastly different magnitudes by normalizing.
  \begin{enumerate}
    \item For each row $i = 1 \dots n$, compute the scale factor: $s_i = \max_j \abs{a_{ij}}$
    \item For pivot column $k$, choose the row $r$ such that $\frac{\abs{a_{rk}}}{s_r}$ is maximal for $r \geq k$
    \item If $r \neq k$, swap rows: $E_k \leftrightarrow E_r$
    \item Proceed with Gaussian Elimination
  \end{enumerate}

  \subsection{Full Pivoting}
  Most stable but most expensive. Swap both rows and columns.
  \begin{enumerate}
    \item At step $k$, find the largest element $|a_{ij}|$ in the submatrix $A_{k:n,k:n}$
    \item Swap row $k$ with row $i$, and column $k$ with column $j$
    \item Update row and column permutations
    \item Continue Gaussian Elimination
  \end{enumerate}

  \section{Linear Algebra}
  \begin{itemize}
    \item To multiply $A \cdot B$, dot-product the rows of $A$ by the columns of $B$.
    \item $A A^{-1} = A^{-1} A = I$
    \item To find $A^{-1}$, row reduce the aug. matrix $[A | I]$.
    \item $A^T$ is $A$ flipped over the main diagonal.
  \end{itemize}

  \subsection{Determinant}
  \begin{itemize}
    \item $\det(A) \neq 0 \implies \begin{cases}
        A^{-1} & \text{exists} \\
        Ax = b & \text{has a unique solution}
      \end{cases}$
    \item \textbf{Cofactor Expansion (Laplace Expansion)}:  
      $\det(A) = \sum_{j=1}^{n} a_{ij} (-1)^{i+j} \det(A_{ij})$

      
  \end{itemize}

  \section{Matrix Factorization}
  \subsection{LU Decomposition}
  If Gaussian elimination can be performed without row exchanges:
  $A = LU$,
  where $ L $ is lower triangular with unit diagonal entries and $ U $ is upper triangular.

  To solve $ Ax = b $:
  \begin{enumerate}
    \item Solve $ Ly = b $ via forward substitution.
    \item Solve $ Ux = y $ via backward substitution.
  \end{enumerate}
  \textbf{Cost:} $ O(n^3) $ for factorization, $ O(n^2) $ per solve.

  \textbf{Row Swaps}:
  If row swaps are needed, introduce a permutation matrix 
  $P:PA = LU \Rightarrow A = P^{-1}LU$,
  Then solve: $ LUx = Pb $

  \section{Special Matrices}
  \subsection{Permutation Matrices}
  \begin{itemize}
    \item Formed by permuting rows of $I_n$, So there is exactly one entry of 1
      per row and column.
    \item $P^{-1} = P^\top$
    \item $PA$ permutes rows of $A$.
  \end{itemize}

  \subsection{Singular}
  \begin{itemize}
    \item A matrix $A$ is singular if $\det(A) = 0$.
    \item Not invertible; $ Ax = b $ has either no solution or infinitely many.
  \end{itemize}

  \subsection{Banded Matrices}
  \begin{itemize}
    \item Nonzero entries confined to a diagonal band.
    \item If $|i - j| > w \Rightarrow a_{ij} = 0$, bandwidth = $w$.
    \item Common in finite difference methods and sparse linear systems.
  \end{itemize}

  \subsection{Tridiagonal Matrices}
  \begin{itemize}
    \item Banded matrix with $w=1$ (main $\pm 1$ diagonals).
    \item Nonzero entries only on the main diagonal and the first sub/super
      diagonals.
    % \item Efficiently solvable using Thomas algorithm in $O(n)$.
  \end{itemize}

  \subsection{Diagonally Dominant (DD / SDD)}
  \begin{itemize}
    \item $ A $ is strictly diagonally dominant if: \newline
      $\displaystyle|a_{ii}| > \sum_{j \ne i} |a_{ij}| \quad \forall i$
    \item $A$ is weakly diagonally dominant of $\abs{a_{ii}} \boldsymbol{\geq} ...$
    \item Guarantees LU factorization without row swaps.
    \item Guaranteed convergence of Jacobi and G-S.
  \end{itemize}

  \subsection{Symmetric Positive Definite (SPD)}
  \begin{itemize}
    \item $ A $ is positive definite if
      $\forall x \ne 0 : x^T A x > 0$
    \item All eigenvalues are positive.
    \item All leading principal minors are positive. $\forall k\det(A_{1:k, 1:k}) > 0$
    \item Cholesky factorization: $A = LL^T$ lets us solve $Ax=b$ in $O(n^2)$
      time.
    \item Also: $A = LDL^T$
  \end{itemize}

  \section{Iterative Methods for Linear Systems}

  \subsection{Convergent Matrix Theorem}
  The following statements are equivalent:
  \begin{enumerate}[label=(\roman*)]
    \item $A$ is convergent
    \item $\rho(A) < 1$ (nec + suf for Jacobi and G-S)
    \item $\forall x: \lim_{n \to \infty} A^nx = 0$
    \item $\forall \alpha: \lim_{n\to\infty} \norm{A^n}_\alpha = 0$
  \end{enumerate}

  \subsection{Jacobi Method $A = D + L + U$}
  \[
    x^{(k+1)} = \underbrace{D^{-1}(L+U)}_{T_J}x^{(k)} 
    + \underbrace{D^{-1}b}_{C_J}
  \]
  \begin{itemize}
      % $x^{(k+1)} = D^{-1}(b - (L + U)x^{(k)})$ 
    \item Requires $a_{ii} \ne 0$. Always permute so $a_{ii}$ big.
    \item Uses previous iteration values for all components.
    \item Converges if $A$ \textbf{strictly} diagonally dominant or SPD.
  \end{itemize}

  \subsection{Gauss-Seidel Method $A = D + L + U$}
  \[
    x^{(k+1)} = \underbrace{(D + L)^{-1}U}_{T_{GS}}x^{(k)} 
    + \underbrace{(D + L)^{-1}Lb}_{C_{GS}}
  \]
  \begin{itemize}
    \item Iteration uses most recent updates:
    \item Often converges faster than Jacobi.
    \item Also converges under \textbf{strict} diagonal dominance or SPD.
  \end{itemize}

  \section{Numerical Interpolation}

  \subsection{Lagrange Interpolation}
  Constructs a polynomial $P(x)$ of degree $\leq n$ through points $(x_0, y_0), \dots, (x_n, y_n)$:
  \begin{align*}
    P(x) &= \sum_{j=0}^n y_j L_j(x) \\
    L_j(x) &= \prod_{\substack{0 \le i \le n \\ i \ne j}} \frac{x - x_i}{x_j - x_i}
  \end{align*}
  
  \textbf{Error:} If $f \in C^{n+1}[a, b]$, then
  \begin{align*}
    f(x) &= P(x) + \frac{f^{(n+1)}(\xi)}{(n+1)!} \prod_{i=0}^n (x - x_i)\\ 
    &\quad \text{for some } \xi \in [a, b]
  \end{align*}
  
  \subsection{Newton's Divided Differences}
  Efficient and updatable polynomial form:
  \begin{align*}
    P(x) &= f[x_0] + f[x_0,x_1](x - x_0) + \dots \\
         &+ f[x_0,\dots,x_n](x - x_0)\dots(x - x_{n-1})
  \end{align*}
  
  Recursive definition:
  \begin{itemize}
  \item zeroth: $f[x_0] = f(x_0)$
  \item first: $f[x_0,x_1] = \frac{f(x_1) - f(x_0)}{x_1 - x_0}$
  \item $k$th: 
    $f[x_i, \dots, x_{i+k}] &= \frac{f[x_{i+1}, \dots, x_{i+k}] - f[x_i, \dots,
    x_{i+k-1}]}{x_{i+k} - x_i}$

    % $f[x_0,\dots,x_k] = \frac{f(x_{k+1},\dots,x_k) - f(x_0,\dots,x_{k-1})}{x_k - x_0}$
  \end{itemize}


  % \begin{align*}
  %   f[x_i,x_{i+1}] &= \frac{f[x_{i+1}] - f[x_i]}{x_{i+1} - x_i} \\
  % \end{align*}
  

  \subsection{Neville's Method}
  Recursive algorithm to evaluate $P(x)$ at a point:
  \[
    P_{i,j}(x) = \frac{(x - x_i) P_{i+1,j}(x) - (x - x_j) P_{i,j-1}(x)}{x_j - x_i}
  \]
  Returns $P(x)$ only â€” not the polynomial form.

  \subsection{Hermite Interpolation}
  Matches both values and derivatives:
  - Duplicate nodes in divided difference table.
  - Derivative at a node: $f[x_i, x_i] = f'(x_i)$.

  \subsection{Cubic Spline Interpolation}
  Piecewise cubic $S_i(x)$ defined on $[x_i, x_{i+1}]$:
  \begin{itemize}
    \item $S(x)$, $S'(x)$, and $S''(x)$ are continuous.
    \item \textbf{Natural spline:} $S''(x_0) = S''(x_n) = 0$.
    \item Solve a tridiagonal linear system for coefficients.
  \end{itemize}

  \subsection{Parametric Curves}
  For 2D/3D data: interpolate $x(t)$, $y(t)$, $z(t)$ independently.
  Used in animation and CAD. Preserves geometric continuity.

  \section{Numerical Integration}

  \subsection{Trapezoidal Rule}
  \begin{itemize}
    \item Approximates $f(x)$ with a linear polynomial over $[a,b]$:
      \[
        \int_a^b f(x)\,dx \approx \frac{h}{2} \qty[f(x_0) + f(x_1)]
      \]
    \item Composite version over $n$ subintervals ($h = \frac{b-a}{n}$):
      \[
        \int_a^b f(x)\,dx \approx \frac{h}{2} \qty[f(x_0) + 2 \sum_{j=1}^{n-1} f(x_j) + f(x_n)]
      \]
    \item Error: $-\frac{(b-a)^3}{12n^2}f^{(2)}(\xi)$ for some $\xi \in [a,b]$
  \end{itemize}

  \subsection{Simpson's Rule}
  \begin{itemize}
    \item Approximates $f(x)$ with a quadratic polynomial:
      \[
        \int_{x_0}^{x_2} f(x)\,dx \approx \frac{h}{3} \qty[f(x_0) + 4f(x_1) + f(x_2)]
      \]
    \item Composite version (even $n$):
      \[
        \int_a^b f(x)\,dx \approx \frac{h}{3} \qty[
        f(x_0) +
        2\sum_{j=1}^{n/2 - 1} f(x_{2j}) +
        4\sum_{j=1}^{n/2} f(x_{2j - 1}) +
        f(x_n)
        ]
      \]
    \item Error: $-\frac{(b-a)^5}{180n^4}f^{(4)}(\xi)$ for some $\xi \in [a,b]$
  \end{itemize}

  \section{ODE Initial Value Problems}

  \subsection{Euler's Method}
  \begin{align*}
    w_{n+1} &= w_n + h f(t_n, w_n) \\
    \text{Error: } & O(h)
  \end{align*}

  \subsection{Modified Euler Method (Heun's)}
  \begin{align*}
    w_{n+1} &= w_n + \frac{h}{2} \qty[f(t_n, w_n) + f(t_{n+1}, w_n + h f(t_n, w_n))] \\
    \text{Error: } & O(h^2)
  \end{align*}

  \subsection{Midpoint Method}
  \begin{align*}
    w_{n+1} &= w_n + h f\qty(t_n + \frac{h}{2}, w_n + \frac{h}{2} f(t_n, w_n)) \\
    \text{Error: } & O(h^2)
  \end{align*}

  \subsection{Runge-Kutta Method (RK4)}
  \begin{align*}
    k_1 &= h f(t_n, w_n) \\
    k_2 &= h f\qty(t_n + \frac{h}{2}, w_n + \frac{k_1}{2}) \\
    k_3 &= h f\qty(t_n + \frac{h}{2}, w_n + \frac{k_2}{2}) \\
    k_4 &= h f(t_n + h, w_n + k_3) \\
    w_{n+1} &= w_n + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\
    \text{Error: } & O(h^4)
  \end{align*}
\end{multicols*}

\end{document}
