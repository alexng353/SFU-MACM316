\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{wasysym}
\usepackage{ulem}
\usepackage{xspace}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{cleveref}

\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Nul}{Nul}
\DeclareMathOperator{\Row}{Row}
\DeclareMathOperator{\proj}{proj}

\setlength{\arraycolsep}{12pt}

\newcommand{\defn}{\textbf{Def.}\xspace}
\newcommand{\thm}{\textbf{Thm.}\xspace}
\newcommand{\ex}{\textbf{ex.}\xspace}
\newcommand{\Ex}{\textbf{Ex.}\xspace}
\newcommand{\ie}{\textbf{i.e.}\xspace}
\newcommand{\lemma}{\textit{Lemma}\xspace}
\newcommand{\bproof}{\textit{Proof ($\impliedby$).}\xspace}
\newcommand{\fproof}{\textit{Proof ($\implies$).}\xspace}
\newcommand{\bigEps}{\mathcal{E}}
\newcommand{\soln}{\textit{Soln.}\xspace}

\renewcommand{\arraystretch}{1.25} % Adjust row spacing


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
}

\newcommand{\ulhref}[2]{\href{#1}{\color{blue}\uline{#2}}}

\begin{document}

\title{MACM 316 Lecture 26 - Chapter 4}
\author{Alexander Ng}
\date{Monday, March 17, 2025}

\maketitle

\section*{Key Takeaways from this Lecture}

Be aware that most of what we did in this chapter is looking over formulas for
approximating derivatives. So this entire notes package is just math. The key
takeaways are just point forms of what we went over.

\begin{enumerate}
  \item Numerical Differentiation (\ref{sec:numerical_differentiation})
    \begin{enumerate}[label=(\alph*)]
      \item We approximate derivatives using interpolation of easier functions 
        (\ex Lagrange polynomials)
      \item Simplest case: using two points to approximate the first derivative
      \item The \textbf{Forward Difference Formula} (for $h>0$) and
        \textbf{Backward Difference Formula} (for $h<0$) are basic numerical
        differentiation methods \eqref{eq:difference_formula}
    \end{enumerate}
  \item More General Approximation Formulas (\ref{sec:more_general_approximation_formulas})
    \begin{enumerate}[label=(\alph*)]
      \item Using $(n+1)$-point formulas, we get more accurate derivative
        approximations
      \item These formulas all follow from differentiating Lagrange polynomials
      \item Three, five and higher-order point formulas are commonly used
      \item Formula reference: \eqref{eq:three_point_formula}. The error term
        depends on the highest derivative of $f(x)$ and the spacing of the
        nodes.
    \end{enumerate}
  \item Equally Spaced Three-Point Formulas (\ref{sec:equally_spaced_three_point_formulas})
    \begin{enumerate}[label=(\alph*)]
      \item When the points are evenly spaced, the formulas for approximating
        derivatives become \eqref{eq:three_point_formula_1}, \eqref{eq:three_point_formula_2}
        and \eqref{eq:three_point_formula_3}
      \item These three formulas correspond to 
        \begin{enumerate}
          \item Forward Difference (uses two points ahead)
          \item Centered Difference (uses one point in front and one point behind)
          \item Backward Difference (uses two points behind)
        \end{enumerate}
      \item Check out Example~1 (Section~\ref{sec:example_1})
    \end{enumerate}
  \item Second Derivative Approximation (\ref{sec:second_derivative_approximation})
    \begin{enumerate}[label=(\alph*)]
      \item We approximate $f''(x)$ by using function values at three points.
      \item The formulas are derived using Taylor series expansion: \eqref{eq:second_derivative_approximation}
    \end{enumerate}
  \item Remarks on Error \ref{sec:remarks_on_error}
    \begin{enumerate}[label=(\alph*)]
      \item \textbf{Small $h$ leads to large roundoff errors.}
      \item Since numerical differentiation involves dividing by $h$ and higher
        orders of $h$, extremely small $h$ values can exaggerate computational
        errors.
      \item Typically, beyond $h\approx 10^{-6}$, roundoff errors dominate the
        calculation.
    \end{enumerate}
  \item Richardson's Extrapolation \ref{sec:richardson_extrapolation}
    \begin{enumerate}[label=(\alph*)]
      \item If the error depends on some parameter (such as the step size $h$)
        and the dependency of the error is predictable (we know the erroe
        behaviour), we can extrapolate to obtain more accurate approximations.
      \item We use multiple approximations at different step sizes and
        \textbf{combine them to cancel error terms}.
      \item There's a step by step process here: \eqref{eq:*}, \eqref{eq:**},
        \eqref{eq:?}, \eqref{eq:***}, \eqref{eq:****}, \eqref{eq:???}
      \item This process gives increasing accuracy with each step.
    \end{enumerate}
  \item Final Remarks
    \begin{enumerate}[label=(\alph*)]
      \item Numerical differentiation can \textbf{amplify errors}, so careful
        step-size selection is crucial.
      \item Centered difference methods are generally more accurate than forward
        or backward differences.
      \item \textbf{Richardson's Extrapolation} is a systematic way to improve
        accuracy.
      \item In numerical integration (covered in a later part of this chapter),
        we will integrate the Lagrange polynomial instead of differentiating it.
    \end{enumerate}
  \item Also check out Random Stuff Steve said during lecture \ref{sec:random_stuff}
\end{enumerate}

\section{Numerical Differentiation (C4*1-17.1)}\label{sec:numerical_differentiation}

We also need to approximate the derivatives of functions. One approach is to
differentiate Lagrange polynomial approximations.

Suppose $x_0, x \in (a, b)$ and $f\in C^2[a,b]$.

Now

\begin{align*}
    f(x) &= P_{0,1}(x) + \frac{1}{2!} (x - x_0)(x - x_1) f''(\xi(x)) \\
         &= \frac{f(x_0)(x - x_1)}{x_0 - x_1} + \frac{f(x_1)(x - x_0)}{x_1 - x_0} 
         + \frac{(x - x_0)(x - x_1)}{2!} f''(\xi(x)) \\
         &\text{where } \xi(x) \in [a, b]
\end{align*}

Now differentiate:

\begin{align*}
    f'(x) &= \frac{f(x_1) - f(x_0)}{x_1 - x_0} + D_x \left[ \frac{(x - x_0)(x - x_1)}{2!} f''(\xi(x)) \right] \\
          &= \frac{f(x_1) - f(x_0)}{x_1 - x_0} + \frac{2 (x - x_0)(x - x_1)}{2} f''(\xi(x)) \\
          &\quad + \frac{(x - x_0)(x - x_1)}{2} D_x \left( f''(\xi(x)) \right)
\end{align*}

We only care about the derivatives at the nodes $x_0, x_1$. The last error term
becomes zero.

\pagebreak
For example, at $x=x_0$,

\[
f'(x_0) = \frac{f(x_1) - f(x_0)}{x_1 - x_0} - \frac{(x_0 - x_1)}{2} f''(\xi)
.\]

Typically, we set $x_1 = x_0+h$, then
\begin{equation}
  f'(x_0) = \frac{f(x_0+h) - f(x_0)}{h} - \frac{h}{2} f''(\xi(x_0))
  \label{eq:difference_formula}
\end{equation}

This is known as a \uline{forward difference formula} if $h>0$ and a \uline{backward
difference formula} if $h<0$. We can derive more general approximation formulas:

\subsection{More General Approximation Formulas (17.3)}\label{sec:more_general_approximation_formulas}

Suppose $x_0, x_1, \dots x_n \in (a,b)$ and $f\in C^{n+1}[a,b]$. Now

\begin{align*}
  f(x) &= \sum_{k=0}^n f(x_k) L_k(x) \\
  &\quad+ \frac{(x-x_0)\dots (x-x_n)}{(n+1)!} f^{(n+1)}(\xi(x))
\end{align*}

for some $\xi(x) \in [a,b]$

Differentiate and evaluate at $x=x_j$:

\begin{align*}
  f'(x_j) &= \sum_{k=0}^n f(x_k) L_k'(x_j) \\
          &\quad+ \frac{f^{(n+1)}(\xi(x_j))}{(n+1)!} \prod_{k=0; k\neq j}^{n} (x_j - x_k)
          \label{eq:three_point_formula}
\end{align*}

This is an $(n+1)$ point formula for $f'(x_j)$ since we use the $(n+1)$ values
$f(x_k); k=0,\dots,n$. Two, three and five point formulas are hte most commonly
used formulas.

\subsubsection{Three Point Formulas (17.4)}

Consider $3$ point formulas with $x_0, x_1$ and $x_2$.

Given \( n = 2 \):

\[
L_0(x) = \frac{(x - x_1)(x - x_2)}{(x_0 - x_1)(x_0 - x_2)}
\]

Taking the derivative:

\[
L_0'(x) = \frac{2x - x_1 - x_2}{(x_0 - x_1)(x_0 - x_2)}
\]

Similarly,

\begin{align*}
    L_1'(x) &= \frac{2x - x_0 - x_2}{(x_1 - x_0)(x_1 - x_2)} \\[10pt]
    L_2'(x) &= \frac{2x - x_0 - x_1}{(x_2 - x_0)(x_2 - x_1)}
\end{align*}

\noindent and

\begin{align*}
    f'(x_j) &= f[x_0] \left[ \frac{2x_j - x_1 - x_2}{(x_0 - x_1)(x_0 - x_2)} \right] \\[10pt]
    &\quad + f[x_1] \left[ \frac{2x_j - x_0 - x_2}{(x_1 - x_0)(x_1 - x_2)} \right] \\[10pt]
    &\quad + f[x_2] \left[ \frac{2x_j - x_0 - x_1}{(x_2 - x_0)(x_2 - x_1)} \right] \\[10pt]
    &\quad + \frac{1}{6} f^{(3)}(\xi_j) \prod_{\substack{k=0 \\ k \neq j}}^{2} (x_j - x_k)
\end{align*}

These simplify considerably when nodes are equally spaced:

\subsubsection{Equally Spaced Three Point Formulas (17.5)}\label{sec:equally_spaced_three_point_formulas}

Given:
\[
x_1 = x_0 + h, \quad x_2 = x_0 + 2h
\]

\begin{align}
    f'(x_0) &= \frac{1}{h} \left[ -\frac{3}{2} f(x_0) + 2f(x_0 + h) - \frac{1}{2} f(x_0 + 2h) \right] 
    + \frac{h^2}{3} f^{(3)}(\xi_0)
    \label{eq:three_point_formula_1}\\[10pt] 
    f'(x_1) &= \frac{1}{h} \left[ -\frac{1}{2} f(x_1 - h) + \frac{1}{2} f(x_1 + h) \right] 
    - \frac{h^2}{6} f^{(3)}(\xi_1) 
    \label{eq:three_point_formula_2}\\[10pt]
    f'(x_2) &= \frac{1}{h} \left[ \frac{1}{2} f(x_2 - 2h) - 2f(x_2 - h) + \frac{3}{2} f(x_2) \right] 
    + \frac{h^2}{3} f^{(3)}(\xi_2)
    \label{eq:three_point_formula_3}
\end{align}

For convenience, replace $x_1$ and $x_2$ by $x_0$. This gives 3 formulas for
approximating $f'(x_0)$.

\begin{align*}
    f'(x_0) &= \frac{1}{h} \left[ -\frac{3}{2} f(x_0) + 2 f(x_0 + h) - \frac{1}{2} f(x_0 + 2h) \right] 
    + \frac{h^2}{3} f^{(3)}(\xi_0) \\[10pt]
    f'(x_0) &= \frac{1}{h} \left[ -\frac{1}{2} f(x_0 - h) + \frac{1}{2} f(x_0 + h) \right] 
    - \frac{h^2}{6} f^{(3)}(\xi_1) \\[10pt]
    f'(x_0) &= \frac{1}{h} \left[ \frac{1}{2} f(x_0 - 2h) - 2 f(x_0 - h) + \frac{3}{2} f(x_0) \right] 
    + \frac{h^2}{3} f^{(3)}(\xi_2)
\end{align*}

Formula 1 uses two points ahead, formula 2 uses one point in front and one point
behind, and formula 3 uses two points behind.

\pagebreak
\subsubsection{Example 1 (17.5.1)}\label{sec:example_1}

\Ex use the most appropriate three-point formula to determine approximatinos
that will complete the following table:

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        $x$  & $f(x)$    & $f'(x)$ \\ 
        \hline
        1.1  & 9.025013  &  \\ 
        1.2  & 11.02318  &  \\ 
        1.3  & 13.46374  &  \\ 
        1.4  & 16.44465  &  \\ 
        \hline
    \end{tabular}
    % \caption{Table of values for $x$, $f(x)$, and $f'(x)$.}
    % \label{tab:values}
\end{table}

\uline{ANSWER: (17.6)}

We only have data in $[1.1, 1.4]$, and no data before or after this interval.
So, for $f'(1.1)$ we use formula 1. For $f'(1.2)$ we can use formula 2 or
formula 1, but since formula 2 has a smaller error, we choose it. We do the same
for $f'(1.3)$. For $f'(1.4)$ we use formula 3, since we only have data behind,
and no available data ahead.

\begin{align*}
    f'(1.1) &\approx \frac{1}{2(0.1)} \left[ -3f(1.1) + 4f(1.2) - f(1.3) \right] = 17.769705 \\[10pt]
    f'(1.2) &\approx \frac{1}{2(0.1)} \left[ f(1.3) - f(1.1) \right] = 22.193635 \\[10pt]
    f'(1.3) &\approx \frac{1}{2(0.1)} \left[ f(1.4) - f(1.2) \right] = 27.107350 \\[10pt]
    f'(1.4) &\approx \frac{1}{2(0.1)} \left[ f(1.2) - 4f(1.3) + f(1.4) \right] = 32.510850
\end{align*}

\textit{*these are OCR'd through ChatGPT. They may be wrong.}

Notice that at the endpoints we must use one sided difference formulas. In the
interior, we used centered differencing. Centered differences often have a
smaller error constant when $f$ is smooth, and they require fewer operations to
compute, but they cannot be used at the endpoints.

\subsubsection{The Second Derivative of $f$ (17.8)}\label{sec:second_derivative_approximation}
Approximations to higher order derivatives may also be found based on function
values. Consider finding the second derivative of $f$.

\begin{align*}
    f(x_0 + h) &= f(x_0) + f'(x_0) h + \frac{1}{2} f''(x_0) h^2 + \frac{1}{6}
    f'''(x_0) h^3 + \frac{1}{24} f^{(4)}(\xi_1) h^4 \\
    f(x_0 - h) &= f(x_0) - f'(x_0) h + \frac{1}{2} f''(x_0) h^2 - \frac{1}{6}
    f'''(x_0) h^3 + \frac{1}{24} f^{(4)}(\xi_{-1}) h^4
\end{align*}

\noindent where \( x_0 - h < \xi_{-1} < x_0 < \xi_{1} < x_0 + h \).

Summing:
\begin{equation}
    f(x_0 + h) + f(x_0 - h) = 2f(x_0) + f''(x_0) h^2 + \frac{h^4}{24} \left[ f^{(4)}(\xi_1) + f^{(4)}(\xi_2) \right]
    \label{eq:second_derivative_approximation}
\end{equation}

We assume \( f^{(4)} \) is continuous on \( [x_0 - h, x_0 + h] \).

Since
\(
\displaystyle\frac{1}{2} \left[ f^{(4)}(\xi_1) + f^{(4)}(\xi_{-1}) \right]
\)
is between \( f^{(4)}(\xi_1) \) and \( f^{(4)}(\xi_{-1}) \), the 
\textit{intermediate value theorem} implies that there is a number
\( \xi \) between \( \xi_1 \) and \( \xi_2 \) such that
\[
  f^{(4)}(\xi) = \frac{1}{2} \left[ f^{(4)}(\xi_1) + f^{(4)}(\xi_{-1}) \right].
\]

Thus,
\begin{align*}
    f(x_0 + h) + f(x_0 - h) &= 2 f(x_0) + f''(x_0) h^2 + \frac{h^4}{24} f^{(4)}(\xi).
\end{align*}

\noindent Therefore,
\begin{align*}
    f''(x_0) &= \frac{1}{h^2} \left[ f(x_0 - h) - 2 f(x_0) + f(x_0 + h) \right] - \frac{h^2}{24} f^{(4)}(\xi).
\end{align*}

\subsubsection{Remarks on Error (17.9)}\label{sec:remarks_on_error}

Notice that all the differentiation formulas divide by some power of $h$.
Division by small numbers tends to exaggerate roundoff error, but this is an
effect that cannot be entirely avoided in numerical differentiation. Thus we do
not want to take $h$ to be too small because then the roundoff errors will
dominate the calculation.

The exact value at which $h$ becomes too small depends on the scaling of your
problem and your specific function $f$. However, for most problems, scaled
nicely, you will start to see roundoff errors at around $h=10^{-6}$.

\section{Richardson's Extrapolation (C4*1-17.10)}\label{sec:richardson_extrapolation}

When the error depends on some parameter such as the step size $h$ and the
dependency is predictable, we can often derive higher order accuracy from low
order formulas. To illustrate the procedure, assume we have an approximation
$N(h)$ to some quantity $M$. Assume this approximation has an order $h$
truncation error and that we know the expression for the first few terms of the
trunction error,

\begin{equation}
M = N(h) + k_1h + k_2h^2 + k_3h^3 + \dots
\label{eq:*}
\end{equation}

\noindent where the $k_i$'s are constants, $h$ is a positive parameter and
$N(h)$ is an $O(h)$ approximation to $M$. We can repeat the calculation with a
parameter $\frac{h}{2}$. Now:

\begin{equation}
M=N(\frac{h}{2}) + \frac{k_1}{2}h + \frac{k_2}{4}h^2 + \frac{k_3}{8}h^3 + \dots
\label{eq:**}
.\end{equation}

We want to obtain a higher order method by using some combination of these
results. 

Subctracting \eqref{eq:*} from twice \eqref{eq:**} gives:

\begin{equation}
  M = [2N(\frac{h}{2}) - N(h)] + k_2(\frac{h^2}{2} - h^2) + k_3
  (\frac{h^3}{4}-h^3) + \dots
  \label{eq:?}
\end{equation}

which is an $O(h^2)$ approximation formula for $M$. For ease of notiation, 

\[
\text{Let } N_2(h) = 2N(\frac{h}{2}) - N(h)
.\]

Now 

\begin{equation}
  M = N_2(h) - \frac{1}{2} k_2 h^2 - \frac{3}{4} k_3 h^3 - \dots
  \label{eq:***}
\end{equation}

We can repeat this calculation with $\frac{h}{2}$:

Now 

\begin{equation}
  M = N_2(\frac{h}{2}) -\frac{1}{8} k_2 h^2 - \frac{3k^3}{32}h^3 - \dots
  \label{eq:****}
  \end{equation}

We want to eliminate the $h^2$ term. We can do this by subtracting four times 
\eqref{eq:***} from \eqref{eq:****}, which gives:

\begin{equation}
  3M = 4N_2(\frac{h}{2}) -N_2(h) + \frac{3k_3}{8}h^3 + \dots
  \label{eq:???}
\end{equation}

which gives an $O(h^3)$ formula for approximating $M$. 

\begin{align*}
    M &= N_3(h) + \frac{1}{3} J_3 h^3 + \dots
\end{align*}

where
\begin{align*}
    N_3(h) &= \frac{4}{3} N_2 \left(\frac{h}{2}\right) - \frac{1}{3} N_2(h).
\end{align*}

Similarly, an \( O(h^4) \) approximation can be derived as:
\begin{align*}
    N_4(h) &= N_3 \left(\frac{h}{2}\right) + \frac{N_3(h/2) - N_3(h)}{7}.
\end{align*}

And an \( O(h^5) \) approximation:
\begin{align*}
    N_5(h) &= N_4 \left(\frac{h}{2}\right) + \frac{N_4(h/2) - N_4(h)}{15}.
\end{align*}

Generally, if \( M \) can be written as:
\begin{align*}
    M = N(h) + \sum_{j=1}^{m-1} K_j h^j + O(h^m),
\end{align*}
then for each \( j = 2, 3, \dots, m \), we have an \( O(h^j) \) approximation of the form:
\begin{align*}
    N_j(h) = N_{j-1} \left(\frac{h}{2}\right) + \frac{N_{j-1} \left(\frac{h}{2}\right) - N_{j-1}(h)}{2^{j-1} - 1}.
\end{align*}

\subsection{More remarks - things that Steve said during lecture}\label{sec:random_stuff}
Chapter 4 also considers numerical integration. We know how to integrate
polynomials, so we can just integrate the lagrange polynomial and call it a day.

\end{document}
